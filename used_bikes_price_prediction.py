# -*- coding: utf-8 -*-
"""Used_Bikes_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-lOONFTARllce2JsOmmMEky84CD0rpd-

# Used Bikes Price Prediction
**Goal:** Predict used bike selling price using supervised regression

**Target Variable:** `price`
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/BikePriceProject/dataset/Used_Bikes.csv')
df.head()

# 1. IMPORTS, SETTINGS & HELPER FUNCTIONS

# Core libraries
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocessing
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR

# Metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Feature importance
from sklearn.inspection import permutation_importance

# Model persistence
import joblib

# Display settings
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', '{:.2f}'.format)
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette('husl')

# Random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("All libraries imported successfully!")
print(f"Current Year: {datetime.now().year}")

# HELPER FUNCTIONS

def calculate_metrics(y_true, y_pred):
    """Calculate MAE, RMSE, and R2 for given predictions."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    """Evaluate a model on train and test sets."""
    # Training predictions
    y_train_pred = model.predict(X_train)
    train_metrics = calculate_metrics(y_train, y_train_pred)

    # Test predictions
    y_test_pred = model.predict(X_test)
    test_metrics = calculate_metrics(y_test, y_test_pred)

    print(f"\n{'='*50}")
    print(f"Model: {model_name}")
    print(f"{'='*50}")
    print(f"Training - MAE: {train_metrics['MAE']:,.2f}, RMSE: {train_metrics['RMSE']:,.2f}, R2: {train_metrics['R2']:.4f}")
    print(f"Testing  - MAE: {test_metrics['MAE']:,.2f}, RMSE: {test_metrics['RMSE']:,.2f}, R2: {test_metrics['R2']:.4f}")

    return {
        'Model': model_name,
        'Train_MAE': train_metrics['MAE'],
        'Train_RMSE': train_metrics['RMSE'],
        'Train_R2': train_metrics['R2'],
        'Test_MAE': test_metrics['MAE'],
        'Test_RMSE': test_metrics['RMSE'],
        'Test_R2': test_metrics['R2']
    }

def safe_column_check(df, column_name):
    """Check if a column exists in the dataframe."""
    return column_name in df.columns

print("Helper functions defined!")

"""---
## 2. Problem Definition

**Business Problem:** Predict the selling price of used bikes based on their characteristics.

**Type:** Supervised Regression

**Target Variable:** `price` (continuous)

**Use Cases:**
- Help sellers price their bikes competitively
- Help buyers identify good deals
- Provide insights for dealerships on pricing strategies
"""

# 3. DATASET UNDERSTANDING

import pandas as pd

DATA_PATH = '/content/drive/MyDrive/BikePriceProject/dataset/Used_Bikes.csv'

try:
    df = pd.read_csv(DATA_PATH)
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print(f"ERROR: File not found at {DATA_PATH}")
    raise

# Create a copy for processing
df_original = df.copy()

print(f"\nDataset Shape: {df.shape[0]} rows x {df.shape[1]} columns")

# First few rows
print("First 10 rows of the dataset:")
df.head(10)

# Dataset info
print("\nDataset Information:")
print("="*50)
df.info()

# Data types
print("\nColumn Data Types:")
print("="*50)
print(df.dtypes)

# Identify numeric vs categorical columns dynamically
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print("\nColumn Classification:")
print("="*50)
print(f"Numeric Columns ({len(numeric_cols)}): {numeric_cols}")
print(f"Categorical Columns ({len(categorical_cols)}): {categorical_cols}")

# Statistical summary for numeric columns
print("\nStatistical Summary (Numeric):")
print("="*50)
df[numeric_cols].describe()

# Categorical columns value counts
print("\nCategorical Columns Summary:")
print("="*50)
for col in categorical_cols:
    print(f"\n{col}: {df[col].nunique()} unique values")
    print(df[col].value_counts().head(10))

# ==== BUSINESS MEANING OF KEY COLUMNS ====

print("\n" + "="*60)
print("BUSINESS MEANING ASSUMPTIONS FOR KEY COLUMNS")
print("="*60)

key_columns_meaning = {
    'price': '''TARGET VARIABLE - The selling price of the used bike in INR.
               Higher prices expected for premium brands, lower kms, fewer owners.''',

    'age': '''Age of the bike in years from manufacturing.
              Older bikes typically depreciate, affecting price negatively.''',

    'year': '''Manufacturing year of the bike.
               Newer bikes (higher year) generally command higher prices.''',

    'kms_driven': '''Total kilometers the bike has been driven (odometer reading).
                     Higher kms = more wear & tear = lower expected price.''',

    'owner': '''Number of previous owners (First Owner, Second Owner, etc.).
                Fewer owners typically means better maintenance = higher price.''',

    'brand': '''Manufacturer/brand of the bike (e.g., Royal Enfield, Honda, Bajaj).
                Premium brands like Triumph, Ducati typically have higher prices.''',

    'power': '''Engine power/displacement in CC.
                Higher CC bikes are generally more expensive.''',

    'city': '''City where the bike is being sold.
               Metropolitan cities may have different pricing dynamics.''',

    'bike_name': '''Full name/model of the bike.
                    High cardinality - may need to be dropped or processed.'''
}

for col, meaning in key_columns_meaning.items():
    if safe_column_check(df, col):
        print(f"\n[{col.upper()}]")
        print(meaning)

# ==== 4. DATA QUALITY AUDIT ====

print("\n" + "="*60)
print("DATA QUALITY AUDIT")
print("="*60)

# Missing values analysis
print("\n--- Missing Values Analysis ---")
missing_count = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df)) * 100
missing_df = pd.DataFrame({
    'Column': missing_count.index,
    'Missing_Count': missing_count.values,
    'Missing_Percent': missing_percent.values
})
missing_df = missing_df.sort_values('Missing_Count', ascending=False)
print(missing_df.to_string(index=False))

# Duplicate rows analysis
print("\n--- Duplicate Rows Analysis ---")
duplicate_count = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")
print(f"Percentage of duplicates: {(duplicate_count/len(df))*100:.2f}%")

if duplicate_count > 0:
    print("\nSample duplicate rows:")
    print(df[df.duplicated(keep=False)].head(10))

# VALIDITY CHECKS

print("\n--- Data Validity Checks ---")
initial_rows = len(df)
issues_found = []

# Check 1: Negative or zero price (invalid)
if safe_column_check(df, 'price'):
    invalid_price = df[df['price'] <= 0]
    if len(invalid_price) > 0:
        issues_found.append(f"Invalid price (<=0): {len(invalid_price)} rows")
        print(f"Found {len(invalid_price)} rows with price <= 0")
    else:
        print("All prices are positive")

# Check 2: Negative kms_driven (invalid)
if safe_column_check(df, 'kms_driven'):
    invalid_kms = df[df['kms_driven'] < 0]
    if len(invalid_kms) > 0:
        issues_found.append(f"Negative kms_driven: {len(invalid_kms)} rows")
        print(f"Found {len(invalid_kms)} rows with negative kms_driven")
    else:
        print("All kms_driven values are non-negative")

# Check 3: Impossible year (future year or too old)
current_year = datetime.now().year
if safe_column_check(df, 'year'):
    invalid_year = df[(df['year'] > current_year) | (df['year'] < 1900)]
    if len(invalid_year) > 0:
        issues_found.append(f"Invalid year: {len(invalid_year)} rows")
        print(f"Found {len(invalid_year)} rows with invalid year")
    else:
        print("All years are valid (if 'year' column exists)")

# Check 4: Negative age (if age column exists)
if safe_column_check(df, 'age'):
    invalid_age = df[df['age'] < 0]
    if len(invalid_age) > 0:
        issues_found.append(f"Negative age: {len(invalid_age)} rows")
        print(f"Found {len(invalid_age)} rows with negative age")
    else:
        print("All age values are non-negative")

# Check 5: Negative power (if power column exists)
if safe_column_check(df, 'power'):
    invalid_power = df[df['power'] <= 0]
    if len(invalid_power) > 0:
        issues_found.append(f"Invalid power (<=0): {len(invalid_power)} rows")
        print(f"Found {len(invalid_power)} rows with power <= 0")
    else:
        print("All power values are positive")

print(f"\nTotal issues found: {len(issues_found)}")
for issue in issues_found:
    print(f"  - {issue}")

# DATA CLEANING

print("\n--- Data Cleaning ---")
print(f"Initial rows: {len(df)}")

# Remove duplicates
# Explanation: Duplicate rows can bias the model and inflate metrics
df = df.drop_duplicates()
print(f"After removing duplicates: {len(df)} rows")

# Remove rows with invalid price (<=0 or NaN)
# Explanation: Target variable must be valid for supervised learning
if safe_column_check(df, 'price'):
    df = df[df['price'] > 0]
    df = df.dropna(subset=['price'])
    print(f"After removing invalid prices: {len(df)} rows")

# Remove rows with negative kms_driven
# Explanation: Odometer readings cannot be negative
if safe_column_check(df, 'kms_driven'):
    df = df[df['kms_driven'] >= 0]
    print(f"After removing negative kms: {len(df)} rows")

# Remove rows with negative age
# Explanation: Age cannot be negative
if safe_column_check(df, 'age'):
    df = df[df['age'] >= 0]
    print(f"After removing negative age: {len(df)} rows")

# Remove rows with invalid power (if exists)
if safe_column_check(df, 'power'):
    df = df[df['power'] > 0]
    print(f"After removing invalid power: {len(df)} rows")

# Handle remaining missing values - drop rows with any NaN
# Explanation: For simplicity, dropping rows with missing values;
# could also impute but dataset is large enough
rows_before = len(df)
df = df.dropna()
print(f"After removing remaining NaN: {len(df)} rows (removed {rows_before - len(df)})")

# Reset index after cleaning
df = df.reset_index(drop=True)
print(f"\nFinal cleaned dataset: {len(df)} rows")
print(f"Rows removed during cleaning: {initial_rows - len(df)} ({((initial_rows - len(df))/initial_rows)*100:.2f}%)")

# 5. EXPLORATORY DATA ANALYSIS (EDA)

print("\n" + "="*60)
print("EXPLORATORY DATA ANALYSIS")
print("="*60)

# Update column lists after cleaning
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Remove target from features list
numeric_features = [col for col in numeric_cols if col != 'price']

print(f"Numeric Features: {numeric_features}")
print(f"Categorical Features: {categorical_cols}")

# UNIVARIATE ANALYSIS

# 1. Price Distribution (Histogram + KDE)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histogram with KDE
axes[0].hist(df['price'], bins=50, edgecolor='black', alpha=0.7, density=True)
df['price'].plot(kind='kde', ax=axes[0], color='red', linewidth=2)
axes[0].set_xlabel('Price (INR)')
axes[0].set_ylabel('Density')
axes[0].set_title('Price Distribution (Histogram + KDE)')
axes[0].axvline(df['price'].mean(), color='green', linestyle='--', label=f'Mean: {df["price"].mean():,.0f}')
axes[0].axvline(df['price'].median(), color='orange', linestyle='--', label=f'Median: {df["price"].median():,.0f}')
axes[0].legend()

# Boxplot
axes[1].boxplot(df['price'], vert=True)
axes[1].set_ylabel('Price (INR)')
axes[1].set_title('Price Boxplot')

plt.tight_layout()
plt.show()

# INSIGHT: Look for skewness - right-skewed distribution indicates most bikes are
# affordable with some luxury outliers. Boxplot shows outliers above upper whisker.

# 2. KMS Driven Distribution
if safe_column_check(df, 'kms_driven'):
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    axes[0].hist(df['kms_driven'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    axes[0].set_xlabel('Kilometers Driven')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('KMS Driven Distribution')
    axes[0].axvline(df['kms_driven'].mean(), color='red', linestyle='--', label=f'Mean: {df["kms_driven"].mean():,.0f}')
    axes[0].legend()

    axes[1].boxplot(df['kms_driven'], vert=True)
    axes[1].set_ylabel('Kilometers Driven')
    axes[1].set_title('KMS Driven Boxplot')

    plt.tight_layout()
    plt.show()

    # INSIGHT: Check for typical mileage range. High-mileage outliers may be
    # commercial bikes or very old vehicles.

# 3. Brand Distribution (Top 15)
if safe_column_check(df, 'brand'):
    fig, ax = plt.subplots(figsize=(12, 6))

    top_brands = df['brand'].value_counts().head(15)
    top_brands.plot(kind='bar', ax=ax, color='teal', edgecolor='black')
    ax.set_xlabel('Brand')
    ax.set_ylabel('Count')
    ax.set_title('Top 15 Bike Brands by Count')
    ax.tick_params(axis='x', rotation=45)

    # Add count labels
    for i, v in enumerate(top_brands):
        ax.text(i, v + 50, str(v), ha='center', fontsize=9)

    plt.tight_layout()
    plt.show()

    # INSIGHT: Identify dominant brands in the market. Popular brands may have
    # more reliable price predictions due to more data points.

# 4. Owner Distribution
if safe_column_check(df, 'owner'):
    fig, ax = plt.subplots(figsize=(10, 5))

    owner_counts = df['owner'].value_counts()
    owner_counts.plot(kind='bar', ax=ax, color='coral', edgecolor='black')
    ax.set_xlabel('Owner Type')
    ax.set_ylabel('Count')
    ax.set_title('Distribution of Bike Ownership History')
    ax.tick_params(axis='x', rotation=0)

    # Add percentage labels
    total = len(df)
    for i, v in enumerate(owner_counts):
        ax.text(i, v + 50, f'{v}\n({v/total*100:.1f}%)', ha='center', fontsize=9)

    plt.tight_layout()
    plt.show()

    # INSIGHT: First owner bikes usually command premium. Check if ownership
    # significantly affects pricing.

# BIVARIATE ANALYSIS

# 1. Price vs KMS Driven (Scatter)
if safe_column_check(df, 'kms_driven'):
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.scatter(df['kms_driven'], df['price'], alpha=0.3, s=10, c='navy')
    ax.set_xlabel('Kilometers Driven')
    ax.set_ylabel('Price (INR)')
    ax.set_title('Price vs Kilometers Driven')

    # Add trend line
    z = np.polyfit(df['kms_driven'], df['price'], 1)
    p = np.poly1d(z)
    x_line = np.linspace(df['kms_driven'].min(), df['kms_driven'].max(), 100)
    ax.plot(x_line, p(x_line), 'r--', linewidth=2, label='Trend Line')
    ax.legend()

    plt.tight_layout()
    plt.show()

    # INSIGHT: Expect negative correlation - higher kms should mean lower price.
    # Look for clusters and outliers.

# 2. Price vs Bike Age (Scatter)
# Create bike_age if 'age' column exists OR calculate from 'year'
if safe_column_check(df, 'age'):
    df['bike_age'] = df['age']
    print("Using existing 'age' column as bike_age")
elif safe_column_check(df, 'year'):
    df['bike_age'] = current_year - df['year']
    print(f"Created bike_age = {current_year} - year")

if safe_column_check(df, 'bike_age'):
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.scatter(df['bike_age'], df['price'], alpha=0.3, s=10, c='darkgreen')
    ax.set_xlabel('Bike Age (Years)')
    ax.set_ylabel('Price (INR)')
    ax.set_title('Price vs Bike Age')

    # Add trend line
    z = np.polyfit(df['bike_age'], df['price'], 1)
    p = np.poly1d(z)
    x_line = np.linspace(df['bike_age'].min(), df['bike_age'].max(), 100)
    ax.plot(x_line, p(x_line), 'r--', linewidth=2, label='Trend Line')
    ax.legend()

    plt.tight_layout()
    plt.show()

    # INSIGHT: Older bikes should depreciate. Look for pattern of price decline
    # with age. Some vintage bikes might be exceptions.

# 3. Price vs Owner (Boxplot)
if safe_column_check(df, 'owner'):
    fig, ax = plt.subplots(figsize=(10, 6))

    # Order by median price descending
    owner_order = df.groupby('owner')['price'].median().sort_values(ascending=False).index

    sns.boxplot(x='owner', y='price', data=df, order=owner_order, ax=ax, palette='Set2')
    ax.set_xlabel('Owner Type')
    ax.set_ylabel('Price (INR)')
    ax.set_title('Price Distribution by Ownership History')
    ax.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    # INSIGHT: First owner bikes typically have higher median price.
    # Each additional owner reduces the price.

# 4. Price vs Top Brands (Boxplot)
if safe_column_check(df, 'brand'):
    fig, ax = plt.subplots(figsize=(14, 6))

    # Get top 15 brands by count
    top_brands_list = df['brand'].value_counts().head(15).index.tolist()
    df_top_brands = df[df['brand'].isin(top_brands_list)]

    # Order by median price
    brand_order = df_top_brands.groupby('brand')['price'].median().sort_values(ascending=False).index

    sns.boxplot(x='brand', y='price', data=df_top_brands, order=brand_order, ax=ax, palette='husl')
    ax.set_xlabel('Brand')
    ax.set_ylabel('Price (INR)')
    ax.set_title('Price Distribution by Top 15 Brands')
    ax.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    # INSIGHT: Premium brands (Triumph, Ducati, Harley) show higher median prices.
    # Budget brands (Hero, Bajaj) show lower and tighter price ranges.

# MULTIVARIATE ANALYSIS

# Correlation Heatmap for Numeric Features
# Update numeric columns list
numeric_cols_for_corr = df.select_dtypes(include=[np.number]).columns.tolist()

fig, ax = plt.subplots(figsize=(10, 8))

correlation_matrix = df[numeric_cols_for_corr].corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Upper triangle mask

sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0,
            fmt='.2f', square=True, linewidths=0.5, ax=ax,
            mask=mask, vmin=-1, vmax=1)
ax.set_title('Correlation Heatmap (Numeric Features)')

plt.tight_layout()
plt.show()

print("\nCorrelation with Price:")
print(correlation_matrix['price'].sort_values(ascending=False))

# INSIGHT: Look for features with strong correlation to price (positive or negative).
# Also check for multicollinearity between features (high correlation between predictors).

# 6. FEATURE ENGINEERING

print("\n" + "="*60)
print("FEATURE ENGINEERING")
print("="*60)

# Create bike_age feature if not already created
if not safe_column_check(df, 'bike_age'):
    if safe_column_check(df, 'age'):
        df['bike_age'] = df['age']
        print("Created bike_age from age column")
    elif safe_column_check(df, 'year'):
        df['bike_age'] = current_year - df['year']
        print(f"Created bike_age = {current_year} - year")
else:
    print("bike_age already exists")

# Identify high-cardinality columns to drop
high_cardinality_threshold = 100
cols_to_drop = []

for col in categorical_cols:
    n_unique = df[col].nunique()
    if n_unique > high_cardinality_threshold:
        cols_to_drop.append(col)
        print(f"Marking '{col}' for removal (high cardinality: {n_unique} unique values)")

# Specifically check for bike_name
if safe_column_check(df, 'bike_name') and 'bike_name' not in cols_to_drop:
    cols_to_drop.append('bike_name')
    print("Marking 'bike_name' for removal (high cardinality noise)")

# Drop high cardinality columns
if cols_to_drop:
    df = df.drop(columns=cols_to_drop, errors='ignore')
    print(f"\nDropped columns: {cols_to_drop}")

# Also drop 'year' if bike_age was created from it (redundant)
if safe_column_check(df, 'year') and safe_column_check(df, 'bike_age'):
    df = df.drop(columns=['year'], errors='ignore')
    print("Dropped 'year' column (redundant with bike_age)")

# Also drop 'age' if bike_age was created from it (redundant)
if safe_column_check(df, 'age') and safe_column_check(df, 'bike_age'):
    df = df.drop(columns=['age'], errors='ignore')
    print("Dropped 'age' column (redundant with bike_age)")

print(f"\nFinal columns: {df.columns.tolist()}")

# PREPARE FEATURES AND TARGET

# Define target
TARGET = 'price'

# Separate features and target
X = df.drop(columns=[TARGET])
y = df[TARGET]

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")

# Dynamically identify numeric and categorical features
numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumeric Features ({len(numeric_features)}): {numeric_features}")
print(f"Categorical Features ({len(categorical_features)}): {categorical_features}")

# CREATE PREPROCESSING PIPELINE

# Numeric transformer: Impute missing + Scale
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Categorical transformer: Impute missing + OneHotEncode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'  # Drop any other columns
)

print("Preprocessing pipeline created!")
print(f"  - Numeric features: StandardScaler")
print(f"  - Categorical features: OneHotEncoder (handle_unknown='ignore')")

# TRAIN-TEST SPLIT

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=RANDOM_STATE
)

print(f"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"\nTarget distribution:")
print(f"  Train - Mean: {y_train.mean():,.2f}, Std: {y_train.std():,.2f}")
print(f"  Test  - Mean: {y_test.mean():,.2f}, Std: {y_test.std():,.2f}")

# 7. BASELINE MODEL - LINEAR REGRESSION

print("\n" + "="*60)
print("BASELINE MODEL: LINEAR REGRESSION")
print("="*60)

# Create pipeline with preprocessing + model
lr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# Fit the model
lr_pipeline.fit(X_train, y_train)

# Store results
results = []

# Evaluate
lr_results = evaluate_model(lr_pipeline, X_train, X_test, y_train, y_test, 'Linear Regression')
results.append(lr_results)

# 8. ADDITIONAL MODELS

print("\n" + "="*60)
print("TRAINING ADDITIONAL MODELS")
print("="*60)

# Dictionary to store fitted models
models = {'Linear Regression': lr_pipeline}

# Model 1: Random Forest Regressor
print("\nTraining Random Forest Regressor...")

rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_STATE,
        n_jobs=-1
    ))
])

rf_pipeline.fit(X_train, y_train)
models['Random Forest'] = rf_pipeline

rf_results = evaluate_model(rf_pipeline, X_train, X_test, y_train, y_test, 'Random Forest')
results.append(rf_results)

# Model 2: Gradient Boosting Regressor
print("\nTraining Gradient Boosting Regressor...")

gb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_STATE
    ))
])

gb_pipeline.fit(X_train, y_train)
models['Gradient Boosting'] = gb_pipeline

gb_results = evaluate_model(gb_pipeline, X_train, X_test, y_train, y_test, 'Gradient Boosting')
results.append(gb_results)

# Model 3: Support Vector Regressor
print("\nTraining Support Vector Regressor...")
print("(Note: SVR can be slow on large datasets)")

svr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', SVR(
        kernel='rbf',
        C=1.0,
        epsilon=0.1
    ))
])

svr_pipeline.fit(X_train, y_train)
models['SVR'] = svr_pipeline

svr_results = evaluate_model(svr_pipeline, X_train, X_test, y_train, y_test, 'SVR')
results.append(svr_results)

# 9. MODEL COMPARISON

print("\n" + "="*60)
print("MODEL COMPARISON")
print("="*60)

# Create comparison DataFrame
comparison_df = pd.DataFrame(results)

# Reorder columns for clarity
comparison_df = comparison_df[['Model', 'Train_MAE', 'Test_MAE', 'Train_RMSE', 'Test_RMSE', 'Train_R2', 'Test_R2']]

# Sort by Test RMSE (ascending = best first)
comparison_df = comparison_df.sort_values('Test_RMSE', ascending=True).reset_index(drop=True)

print("\n" + comparison_df.to_string(index=False))

# Visualize model comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# RMSE comparison
x_pos = np.arange(len(comparison_df))
width = 0.35

axes[0].bar(x_pos - width/2, comparison_df['Train_RMSE'], width, label='Train', color='steelblue')
axes[0].bar(x_pos + width/2, comparison_df['Test_RMSE'], width, label='Test', color='coral')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
axes[0].set_ylabel('RMSE')
axes[0].set_title('RMSE Comparison')
axes[0].legend()

# R2 comparison
axes[1].bar(x_pos - width/2, comparison_df['Train_R2'], width, label='Train', color='steelblue')
axes[1].bar(x_pos + width/2, comparison_df['Test_R2'], width, label='Test', color='coral')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
axes[1].set_ylabel('R² Score')
axes[1].set_title('R² Score Comparison')
axes[1].legend()

# MAE comparison
axes[2].bar(x_pos - width/2, comparison_df['Train_MAE'], width, label='Train', color='steelblue')
axes[2].bar(x_pos + width/2, comparison_df['Test_MAE'], width, label='Test', color='coral')
axes[2].set_xticks(x_pos)
axes[2].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
axes[2].set_ylabel('MAE')
axes[2].set_title('MAE Comparison')
axes[2].legend()

plt.tight_layout()
plt.show()

print(f"\n Best Model (by Test RMSE): {comparison_df.iloc[0]['Model']}")

# 10. HYPERPARAMETER TUNING

print("\n" + "="*60)
print("HYPERPARAMETER TUNING")
print("="*60)

# Store tuned models and results
tuned_models = {}
tuning_results = []

# Tune Random Forest
print("\n--- Tuning Random Forest ---")

rf_param_distributions = {
    'regressor__n_estimators': [50, 100, 200, 300],
    'regressor__max_depth': [5, 10, 15, 20, None],
    'regressor__min_samples_split': [2, 5, 10],
    'regressor__min_samples_leaf': [1, 2, 4],
    'regressor__max_features': ['sqrt', 'log2', None]
}

rf_base = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))
])

rf_search = RandomizedSearchCV(
    rf_base,
    param_distributions=rf_param_distributions,
    n_iter=20,
    cv=5,
    scoring='neg_root_mean_squared_error',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

rf_search.fit(X_train, y_train)

print(f"\nBest Parameters: {rf_search.best_params_}")
print(f"Best CV RMSE: {-rf_search.best_score_:,.2f}")

tuned_models['Random Forest'] = rf_search.best_estimator_

# Evaluate tuned RF
rf_tuned_results = evaluate_model(
    rf_search.best_estimator_,
    X_train, X_test, y_train, y_test,
    'Random Forest (Tuned)'
)
tuning_results.append(rf_tuned_results)

# Tune Gradient Boosting
print("\n--- Tuning Gradient Boosting ---")

gb_param_distributions = {
    'regressor__n_estimators': [50, 100, 200, 300],
    'regressor__max_depth': [3, 5, 7, 10],
    'regressor__learning_rate': [0.01, 0.05, 0.1, 0.2],
    'regressor__min_samples_split': [2, 5, 10],
    'regressor__min_samples_leaf': [1, 2, 4],
    'regressor__subsample': [0.8, 0.9, 1.0]
}

gb_base = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(random_state=RANDOM_STATE))
])

gb_search = RandomizedSearchCV(
    gb_base,
    param_distributions=gb_param_distributions,
    n_iter=20,
    cv=5,
    scoring='neg_root_mean_squared_error',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

gb_search.fit(X_train, y_train)

print(f"\nBest Parameters: {gb_search.best_params_}")
print(f"Best CV RMSE: {-gb_search.best_score_:,.2f}")

tuned_models['Gradient Boosting'] = gb_search.best_estimator_

# Evaluate tuned GB
gb_tuned_results = evaluate_model(
    gb_search.best_estimator_,
    X_train, X_test, y_train, y_test,
    'Gradient Boosting (Tuned)'
)
tuning_results.append(gb_tuned_results)

# Tune SVR (with smaller search space due to slower training)
print("\n--- Tuning SVR ---")

svr_param_distributions = {
    'regressor__C': [0.1, 1, 10, 100],
    'regressor__epsilon': [0.01, 0.1, 0.5, 1.0],
    'regressor__kernel': ['rbf', 'linear'],
    'regressor__gamma': ['scale', 'auto']
}

svr_base = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', SVR())
])

svr_search = RandomizedSearchCV(
    svr_base,
    param_distributions=svr_param_distributions,
    n_iter=15,
    cv=5,
    scoring='neg_root_mean_squared_error',
    random_state=RANDOM_STATE,
    n_jobs=-1,
    verbose=1
)

svr_search.fit(X_train, y_train)

print(f"\nBest Parameters: {svr_search.best_params_}")
print(f"Best CV RMSE: {-svr_search.best_score_:,.2f}")

tuned_models['SVR'] = svr_search.best_estimator_

# Evaluate tuned SVR
svr_tuned_results = evaluate_model(
    svr_search.best_estimator_,
    X_train, X_test, y_train, y_test,
    'SVR (Tuned)'
)
tuning_results.append(svr_tuned_results)

# Compare tuned models
print("\n" + "="*60)
print("TUNED MODELS COMPARISON")
print("="*60)

tuned_comparison_df = pd.DataFrame(tuning_results)
tuned_comparison_df = tuned_comparison_df.sort_values('Test_RMSE', ascending=True).reset_index(drop=True)
print(tuned_comparison_df.to_string(index=False))

# Before vs After tuning comparison
print("\n--- Performance: Before vs After Tuning ---")

comparison_data = []

for model_name in ['Random Forest', 'Gradient Boosting', 'SVR']:
    # Find original results
    orig = next((r for r in results if r['Model'] == model_name), None)
    # Find tuned results
    tuned = next((r for r in tuning_results if model_name in r['Model']), None)

    if orig and tuned:
        improvement = ((orig['Test_RMSE'] - tuned['Test_RMSE']) / orig['Test_RMSE']) * 100
        comparison_data.append({
            'Model': model_name,
            'Original_RMSE': orig['Test_RMSE'],
            'Tuned_RMSE': tuned['Test_RMSE'],
            'Improvement_%': improvement
        })

if comparison_data:
    before_after_df = pd.DataFrame(comparison_data)
    print(before_after_df.to_string(index=False))

# ==== 11. FINAL MODEL ====

print("\n" + "="*60)
print("FINAL MODEL SELECTION")
print("="*60)

# Select best model by Test RMSE from tuned models
best_model_name = tuned_comparison_df.iloc[0]['Model']
best_model_rmse = tuned_comparison_df.iloc[0]['Test_RMSE']

# Extract the base name to get the model
if 'Random Forest' in best_model_name:
    best_model = tuned_models['Random Forest']
    base_name = 'Random Forest'
elif 'Gradient Boosting' in best_model_name:
    best_model = tuned_models['Gradient Boosting']
    base_name = 'Gradient Boosting'
elif 'SVR' in best_model_name:
    best_model = tuned_models['SVR']
    base_name = 'SVR'
else:
    best_model = lr_pipeline
    base_name = 'Linear Regression'

print(f"\n BEST MODEL: {best_model_name}")
print(f"   Test RMSE: {best_model_rmse:,.2f}")

# Final evaluation on test set
print("\n--- Final Model Evaluation on Test Set ---")

y_test_pred_final = best_model.predict(X_test)
final_metrics = calculate_metrics(y_test, y_test_pred_final)

print(f"\nFinal Test Performance:")
print(f"  MAE:  {final_metrics['MAE']:,.2f} INR")
print(f"  RMSE: {final_metrics['RMSE']:,.2f} INR")
print(f"  R²:   {final_metrics['R2']:.4f}")

# Interpretation
avg_price = y_test.mean()
print(f"\nInterpretation:")
print(f"  Average test price: {avg_price:,.2f} INR")
print(f"  MAE as % of avg price: {(final_metrics['MAE']/avg_price)*100:.2f}%")
print(f"  The model explains {final_metrics['R2']*100:.2f}% of the variance in bike prices.")

# 12. MODEL INTERPRETATION

print("\n" + "="*60)
print("MODEL INTERPRETATION")
print("="*60)

# Get feature names after preprocessing
try:
    # Get feature names from the preprocessor
    feature_names = []

    # Numeric feature names
    feature_names.extend(numeric_features)

    # Categorical feature names (after one-hot encoding)
    if categorical_features:
        ohe = best_model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']
        cat_feature_names = ohe.get_feature_names_out(categorical_features).tolist()
        feature_names.extend(cat_feature_names)

    print(f"Total features after preprocessing: {len(feature_names)}")
except Exception as e:
    print(f"Could not extract feature names: {e}")
    feature_names = None

# Feature Importance (for tree-based models)
if base_name in ['Random Forest', 'Gradient Boosting'] and feature_names:
    print("\n--- Feature Importance (Tree-based) ---")

    try:
        importances = best_model.named_steps['regressor'].feature_importances_

        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Importance': importances
        }).sort_values('Importance', ascending=False)

        # Plot top 15
        top_n = min(15, len(importance_df))
        top_features = importance_df.head(top_n)

        fig, ax = plt.subplots(figsize=(10, 8))

        bars = ax.barh(range(top_n), top_features['Importance'].values, color='teal')
        ax.set_yticks(range(top_n))
        ax.set_yticklabels(top_features['Feature'].values)
        ax.invert_yaxis()
        ax.set_xlabel('Importance')
        ax.set_title(f'Top {top_n} Feature Importances ({base_name})')

        # Add value labels
        for bar, val in zip(bars, top_features['Importance'].values):
            ax.text(val + 0.001, bar.get_y() + bar.get_height()/2,
                   f'{val:.4f}', va='center', fontsize=9)

        plt.tight_layout()
        plt.show()

        print("\nTop 15 Features:")
        print(top_features.to_string(index=False))

    except Exception as e:
        print(f"Error extracting feature importance: {e}")

# Permutation Importance (for SVR/Linear or as alternative)
if base_name in ['SVR', 'Linear Regression'] or not feature_names:
    print("\n--- Permutation Importance ---")
    print("(Computing permutation importance - this may take a moment...)")

    try:
        # Transform test data
        X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test)

        # Calculate permutation importance
        perm_importance = permutation_importance(
            best_model.named_steps['regressor'],
            X_test_transformed,
            y_test,
            n_repeats=10,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )

        if feature_names:
            perm_imp_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': perm_importance.importances_mean,
                'Std': perm_importance.importances_std
            }).sort_values('Importance', ascending=False)

            # Plot top 15
            top_n = min(15, len(perm_imp_df))
            top_perm_features = perm_imp_df.head(top_n)

            fig, ax = plt.subplots(figsize=(10, 8))

            ax.barh(range(top_n), top_perm_features['Importance'].values,
                   xerr=top_perm_features['Std'].values, color='coral')
            ax.set_yticks(range(top_n))
            ax.set_yticklabels(top_perm_features['Feature'].values)
            ax.invert_yaxis()
            ax.set_xlabel('Mean Importance')
            ax.set_title(f'Top {top_n} Permutation Importances')

            plt.tight_layout()
            plt.show()

    except Exception as e:
        print(f"Error computing permutation importance: {e}")

# ==== 13. ERROR ANALYSIS ====

print("\n" + "="*60)
print("ERROR ANALYSIS")
print("="*60)

# Calculate residuals
residuals = y_test.values - y_test_pred_final
abs_errors = np.abs(residuals)

# Residuals Distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histogram of residuals
axes[0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')
axes[0].axvline(residuals.mean(), color='green', linestyle='--', linewidth=2,
               label=f'Mean: {residuals.mean():,.0f}')
axes[0].set_xlabel('Residual (Actual - Predicted)')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Residuals Distribution')
axes[0].legend()

# Q-Q plot or residuals vs predicted
axes[1].scatter(y_test_pred_final, residuals, alpha=0.3, s=10, c='navy')
axes[1].axhline(0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Predicted Price')
axes[1].set_ylabel('Residual')
axes[1].set_title('Residuals vs Predicted')

plt.tight_layout()
plt.show()

# INSIGHT: Ideally residuals should be normally distributed around 0.
# Heteroscedasticity (funnel shape in residuals vs predicted) indicates
# the model performs differently across price ranges.

# Actual vs Predicted Scatter
fig, ax = plt.subplots(figsize=(10, 8))

ax.scatter(y_test, y_test_pred_final, alpha=0.3, s=10, c='darkgreen')

# Perfect prediction line
min_val = min(y_test.min(), y_test_pred_final.min())
max_val = max(y_test.max(), y_test_pred_final.max())
ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

ax.set_xlabel('Actual Price (INR)')
ax.set_ylabel('Predicted Price (INR)')
ax.set_title('Actual vs Predicted Prices')
ax.legend()

# Add R² annotation
ax.annotate(f'R² = {final_metrics["R2"]:.4f}',
           xy=(0.05, 0.95), xycoords='axes fraction',
           fontsize=12, bbox=dict(boxstyle='round', facecolor='wheat'))

plt.tight_layout()
plt.show()

# INSIGHT: Points should cluster along the red diagonal line.
# Deviations indicate prediction errors.

# Top 10 Worst Predictions
print("\n--- Top 10 Worst Absolute Errors ---")

# Create error analysis DataFrame
error_analysis = X_test.copy()
error_analysis['Actual_Price'] = y_test.values
error_analysis['Predicted_Price'] = y_test_pred_final
error_analysis['Residual'] = residuals
error_analysis['Absolute_Error'] = abs_errors
error_analysis['Error_Percent'] = (abs_errors / y_test.values) * 100

# Sort by absolute error
worst_predictions = error_analysis.nlargest(10, 'Absolute_Error')

print("\nWorst predictions (highest absolute errors):")
display_cols = ['Actual_Price', 'Predicted_Price', 'Residual', 'Absolute_Error', 'Error_Percent']
# Add feature columns that exist
for col in ['brand', 'owner', 'kms_driven', 'power', 'bike_age', 'city']:
    if col in worst_predictions.columns:
        display_cols.insert(0, col)

print(worst_predictions[display_cols].to_string())

# Error distribution statistics
print("\n--- Error Statistics ---")
print(f"Mean Absolute Error: {np.mean(abs_errors):,.2f} INR")
print(f"Median Absolute Error: {np.median(abs_errors):,.2f} INR")
print(f"Max Absolute Error: {np.max(abs_errors):,.2f} INR")
print(f"Min Absolute Error: {np.min(abs_errors):,.2f} INR")
print(f"\nPercentile Distribution of Absolute Errors:")
for p in [50, 75, 90, 95, 99]:
    print(f"  {p}th percentile: {np.percentile(abs_errors, p):,.2f} INR")

# ==== 14. SAVE ARTIFACTS ====

print("\n" + "="*60)
print("SAVING ARTIFACTS")
print("="*60)

# Save the best model
model_filename = 'best_model.joblib'
try:
    joblib.dump(best_model, model_filename)
    print(f"✓ Best model saved to: {model_filename}")
except Exception as e:
    print(f"✗ Error saving model: {e}")

# Save comparison tables
try:
    # Save main comparison
    comparison_df.to_csv('model_comparison.csv', index=False)
    print("✓ Model comparison saved to: model_comparison.csv")

    # Save tuned comparison
    tuned_comparison_df.to_csv('tuned_model_comparison.csv', index=False)
    print("✓ Tuned model comparison saved to: tuned_model_comparison.csv")
except Exception as e:
    print(f"✗ Error saving CSV: {e}")

# ==== 15. MODEL USAGE EXAMPLE ====

print("\n" + "="*60)
print("MODEL USAGE EXAMPLE")
print("="*60)

# Demonstrate how to load and use the saved model
print("\n# How to load and use the saved model:")
print("""
# Load the model
import joblib
loaded_model = joblib.load('best_model.joblib')

# Make predictions on new data
# new_data should be a DataFrame with the same columns as training data
predictions = loaded_model.predict(new_data)
""")

# Example prediction
print("\nExample: Predicting price for a sample bike:")
sample_bike = X_test.iloc[[0]]
sample_actual = y_test.iloc[0]
sample_pred = best_model.predict(sample_bike)[0]

print(f"\nSample bike features:")
print(sample_bike.to_string())
print(f"\nActual Price: {sample_actual:,.2f} INR")
print(f"Predicted Price: {sample_pred:,.2f} INR")
print(f"Difference: {abs(sample_actual - sample_pred):,.2f} INR")

# ==== FINAL SUMMARY ====

print("\n" + "="*60)
print("PROJECT SUMMARY")
print("="*60)

print(f"""
DATASET:
   - Original samples: {len(df_original):,}
   - After cleaning: {len(df):,}
   - Features used: {len(X.columns)}

PREPROCESSING:
   - Numeric features: {len(numeric_features)} (StandardScaler)
   - Categorical features: {len(categorical_features)} (OneHotEncoder)

MODELS TRAINED:
   - Linear Regression (baseline)
   - Random Forest Regressor
   - Gradient Boosting Regressor
   - Support Vector Regressor

BEST MODEL: {best_model_name}
   - Test MAE: {final_metrics['MAE']:,.2f} INR
   - Test RMSE: {final_metrics['RMSE']:,.2f} INR
   - Test R²: {final_metrics['R2']:.4f}

ARTIFACTS SAVED:
   - best_model.joblib
   - model_comparison.csv
   - tuned_model_comparison.csv
""")

print("\n" + "="*60)
print("PROJECT COMPLETED SUCCESSFULLY!")
print("="*60)